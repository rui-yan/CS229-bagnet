{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[libraries successfully installed...]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "train_on_gpu = True\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "try:\n",
    "    import torchbearer\n",
    "except:\n",
    "    !pip install torchbearer\n",
    "    import torchbearer\n",
    "from torchbearer import Trial\n",
    "import scipy\n",
    "import scipy.special\n",
    "import bagnets.pytorchnet\n",
    "print(\"[libraries successfully installed...]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory\n",
    "data_dir = './flowers_tvtsplit/'\n",
    "\n",
    "# Save our result (model checkpoints, loss_acc data, plots)to this directory\n",
    "saved_model_dir = './model_performance_results/bagnet33_baseline_results/'\n",
    "\n",
    "model_name = 'bagnet33'\n",
    "\n",
    "# Number of classes in  the dataset\n",
    "num_classes = 5\n",
    "\n",
    "# Batch size for training (standardized to BagNet baseline)\n",
    "batch_size = 32\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Using cpu ...]\n"
     ]
    }
   ],
   "source": [
    "#-------------------- Some Helper Functions ---------------------------#\n",
    "\n",
    "# compute gradients for newly initialized layer\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    \"\"\"\n",
    "    This function sets all parameters of model to False, which means we don't fine\n",
    "    tune all parameters but only feature extract and compute gradients\n",
    "    for newly initialized layer.\n",
    "    \"\"\"\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    \"\"\"\n",
    "    This function initializes these variables which will be set in this\n",
    "    if statement. Each of these variables is model specific.\n",
    "    \"\"\"\n",
    "    model_ft = None\n",
    "\n",
    "    if model_name == \"bagnet9\":\n",
    "        model_ft = bagnets.pytorchnet.bagnet9(pretrained=use_pretrained)\n",
    "    if model_name == \"bagnet17\":\n",
    "        model_ft = bagnets.pytorchnet.bagnet17(pretrained=use_pretrained)\n",
    "    if model_name == \"bagnet33\":\n",
    "        model_ft = bagnets.pytorchnet.bagnet33(pretrained=use_pretrained)\n",
    "\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "\n",
    "    # Change the last layer to match our number of classes\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    return model_ft\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[Using\", device , \"...]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> [Preparing data ....]\n",
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "#--------------------- Load test datasets ------------------------------#\n",
    "\n",
    "print(\"==> [Preparing data ....]\")\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),  # resize the image to 224*224 pixels\n",
    "        transforms.CenterCrop(224),  # crop the image to 224*224 pixels about the center\n",
    "        transforms.RandomHorizontalFlip(),  # convert the image to PyTorch Tensor data type\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Just normalization for validation\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_data = torchvision.datasets.ImageFolder(data_dir + \"train/\", data_transforms[\"train\"])\n",
    "val_data = torchvision.datasets.ImageFolder(data_dir + \"val/\", data_transforms[\"val\"])\n",
    "test_data = torchvision.datasets.ImageFolder(data_dir + \"test/\", data_transforms[\"test\"])\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {\"train\": torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                    shuffle=True, num_workers=2),\n",
    "                    \"val\": torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
    "                    shuffle=False, num_workers=2),\n",
    "                    \"test\": torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "                    shuffle=False, num_workers=2)}\n",
    "\n",
    "train_loader = dataloaders_dict['train']\n",
    "val_loader = dataloaders_dict['val']\n",
    "test_loader = dataloaders_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Bagnet-33 model\n",
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "[Using CrossEntropyLoss ...]\n",
      "[Bagnet33 model Initialized...]\n"
     ]
    }
   ],
   "source": [
    "##------------------- Initialize Bagnet-33 model --------------------##\n",
    "\n",
    "print('==> Bagnet-33 model')\n",
    "\n",
    "model_ft = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "# Send the model to CPU\n",
    "model_ft = model_ft.to(device)\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Setup the loss fxn\n",
    "print(\"[Using CrossEntropyLoss ...]\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"[Bagnet33 model Initialized...]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Saved Bagnet33 weights loaded--------------------\n"
     ]
    }
   ],
   "source": [
    "#---------------Load saved weights------------------------#\n",
    "if torch.cuda.is_available():\n",
    "    checkpoint = torch.load(saved_model_dir + \"bagnet33_baseline_model.pth\")\n",
    "else:\n",
    "    checkpoint = torch.load(saved_model_dir + \"bagnet33_baseline_model.pth\", map_location=torch.device('cpu'))\n",
    "    \n",
    "model_ft.load_state_dict(checkpoint['model_bagnet33_state_dict'])\n",
    "optimizer_ft.load_state_dict(checkpoint['optimizer_bagnet33_state_dict'])\n",
    "print(\"--------Saved Bagnet33 weights loaded--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Investigate performance on test datasets---------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1dd97e53894454b626c427dff44e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='0/1(p)', max=14.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0586, -1.4340, -4.4017,  4.4887, -5.0557],\n",
       "        [ 5.4375, -2.2222, -1.6684,  1.8513, -3.9205],\n",
       "        [ 2.6648,  0.5216, -2.2353,  2.2017, -3.7013],\n",
       "        ...,\n",
       "        [-3.3236, -1.4829, -0.9306,  1.9546,  3.1959],\n",
       "        [-1.8625, -1.9189,  0.1799, -1.6414,  4.4501],\n",
       "        [-4.0832, -5.1765,  0.1611, -0.4167,  8.5640]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------------Investigate performance on test datasets---------\n",
    "\n",
    "print(\"--------Investigate performance on test datasets---------\")\n",
    "model_ft.eval()\n",
    "trial = Trial(model_ft, optimizer_ft, criterion, metrics=['loss', 'accuracy']).to(device)\n",
    "trial.with_generators(train_loader, val_generator=val_loader, test_generator=test_loader)\n",
    "predictions = trial.predict()\n",
    "predicted_classes = predictions.argmax(1).cpu()\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 138, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 174, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 155, in pil_loader\n    with open(path, 'rb') as f:\nFileNotFoundError: [Errno 2] No such file or directory: './flowers_tvtsplit/test/tulip/8623173256_3f0eb4c506.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-645fd80b167f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 138, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 174, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 155, in pil_loader\n    with open(path, 'rb') as f:\nFileNotFoundError: [Errno 2] No such file or directory: './flowers_tvtsplit/test/tulip/8623173256_3f0eb4c506.jpg'\n"
     ]
    }
   ],
   "source": [
    "#--------------- Some model performance visualizations & stats ----------\n",
    "'''\n",
    "CREDITS: Code adapted from tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "'''\n",
    "# Define classes\n",
    "classes = ('daisy', 'dandelion', 'rose', 'sunflower', 'tulip')\n",
    "\n",
    "# Check class prediction accuracies\n",
    "dataiter = iter(test_loader)\n",
    "\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "class_correct = list(0. for i in range(5))\n",
    "class_total = list(0. for i in range(5))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_ft(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        if c.size()[0] == 32:\n",
    "            for i in range(32):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "for i in range(5):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
